# -*- coding: utf-8 -*-
"""ECG.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o1BdClDEPmJIWy4kTTUsl5pOr3spkutb
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.utils import to_categorical

# Load in the datasets
train_df = pd.read_csv('/content/drive/My Drive/ECG/mitbih_train.csv', header=None)
test_df = pd.read_csv('/content/drive/My Drive/ECG/mitbih_test.csv', header=None)

# Display the first few rows of the training set and test set
train_df_head = train_df.head()
test_df_head = test_df.head()

print(train_df_head)
print(test_df_head)

#Split the dataset into features and labels (the last column will be used as labels)
X_train = train_df.iloc[:, :-1].values
y_train = train_df.iloc[:, -1].values
X_test = test_df.iloc[:, :-1].values
y_test = test_df.iloc[:, -1].values

# Normalize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Encoding the labels
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

# Split the training data into a training set and a validation set
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
print(X_train.shape, X_val.shape, X_test.shape)
print(y_train.shape, y_val.shape, y_test.shape)

import matplotlib.pyplot as plt

def class_distribution(df, title):
    class_counts = df.iloc[:, -1].value_counts()
    counts = class_counts.values

    # Plot the distribution
    plt.figure(figsize=(8, 8))
    plt.pie(counts, labels=['Normal', 'Supraventricular ectopic', 'Ventricular ectopic', 'Fusion', 'Unknown'], autopct='%1.1f%%', startangle=140)
    plt.title(title)
    plt.show()

# Assuming train_df and test_df are already loaded
class_distribution(train_df, 'Training Set Class Distribution')
class_distribution(test_df, 'Testing Set Class Distribution')

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau

def build_cnn_model(input_shape, num_classes):
    model = Sequential([
        Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=input_shape),
        MaxPooling1D(pool_size=2),
        Dropout(0.1),
        Conv1D(filters=128, kernel_size=5, activation='relu'),
        MaxPooling1D(pool_size=2),
        Flatten(),
        Dense(128, activation='relu'),
        Dropout(0.2),
        Dense(num_classes, activation='softmax')
    ])
    return model

input_shape = (X_train.shape[1], 1)  # Adjusting for Conv1D input
num_classes = y_train.shape[1]
print(input_shape, num_classes)

model = build_cnn_model(input_shape, num_classes)
model.compile(optimizer=Adam(learning_rate=0.0001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

#Since the model requires 3D input, we reshape the inputs
X_train = X_train.reshape((-1, X_train.shape[1], 1))
X_val = X_val.reshape((-1, X_val.shape[1], 1))
X_test = X_test.reshape((-1, X_test.shape[1], 1))

#Define a learning rate monitor for the training process
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)

#Begin training the model
history = model.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_val, y_val), callbacks=[reduce_lr], verbose=1)

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 5))

# Accuracy plot
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='upper left')

# Loss plot
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(loc='upper right')

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"Test accuracy: {test_accuracy}, Test loss: {test_loss}")

import numpy as np
#Make predictions on the test set
predictions = model.predict(X_test, verbose=1)

#Convert probabilities to class predictions
y_pred = np.argmax(predictions, axis=1)
y_true = np.argmax(y_test, axis=1)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import seaborn as sns

# Calculate metrics
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred, average='macro')  # 'macro' for unweighted mean per class, treating all classes equally
recall = recall_score(y_true, y_pred, average='macro')
f1 = f1_score(y_true, y_pred, average='macro')

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1-Score: {f1}")

labels = ['Normal', 'Supraventricular ectopic', 'Ventricular ectopic', 'Fusion', 'Unknown']

import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=labels, yticklabels=labels)

plt.xticks(rotation=45)
plt.yticks(rotation=45)

plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()